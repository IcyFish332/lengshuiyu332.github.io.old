<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Introduction to Probability Theory</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/sky.css">

		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2 style="color: #000000;">Introduction to Probability Theory</h2>
					<p style="font-size: 24px; color: #000000; text-align: center">This presentation will cover the following topics:</p>
					<ul style="font-size: 20px; color: #000000;">
						<li style="color: #4caf50;">Events and Their Probabilities</li>
						<li style="color: #2196f3;">Random Variable</li>
						<li style="color: #ff5722;">Random Vectors</li>
						<li style="color: #9c27b0;">Sequences of Random Variables</li>
					</ul>
				</section>
				<section>
					<h1 style="color: #4caf50;">Events and Their Probabilities</h1>
				</section>
				<section data-markdown>
						## Probabilistic Models

						Elements of a Probabilistic Model:

						- The sample space $\Omega$, the set of all possible outcomes of an experiment.

						- The probability law, assigns to a set $A$ of possible outcomes a nonnegative number $\mathbf{P}(A)$, the probability of $A$.

				</section>
				<section>
					<section data-markdown>
						<textarea data-template>
						## Sample Spaces and Events

						Every probabilistic model involves an underlying process, called the **experiment**, that will produce exactly one out of several possible **outcomes**. The set of all possible outcomes is called the **sample space** of the experiment and is denoted by $\Omega$. A subset of the sample space, that is, a collection of possible outcomes, is called an **event**.
			
						Regardless of their number, different elements of the sample space should be distinct and **mutually exclusive**（互斥） so that when the experiment is carried out there is a unique outcome.
		
						Generally, the sample space chosen for a probabilistic model must be **collectively exhaustive**（完全穷尽）, in the sense that no matter what happens in the experiment, we always obtain an outcome that has been included in the sample space.
						---
						### Events as Sets

						The jargons in set theory and probability theory

`$$
\begin{array}{|l|l|l|}
\hline \text { Typical notation } & \text { Set jargon } & \text { Probability jargon } \\
\hline \Omega & \text { Collection of objects } & \text { Sample space } \\
\hline \omega & \text { Member of } \Omega & \text { Elementary event, outcome } \\
\hline A & \text { Subset of } \Omega & \text { Events that some outcome in } A \text { occurs } \\
\hline A \text { or } \bar{A} & \text { Complement of } A & \text { Event that no outcome in } A \text { occurs } \\
\hline A \cap B & \text { Intersection } & \text { Both } A \text { and } B \\
\hline A \cup B & \text { Union } & \text { Either } A \text { or } B \text { or both } \\
\hline A-B & \text { Difference } & A, \text { but no } B \\
\hline A \subseteq B & \text { Inclusion } & \text { If } A, \text { then } B \\
\hline \emptyset & \text { Empty set } & \text { Impossible event } \\
\hline
\end{array}
$$`
						</textarea>
					</section>
				</section>
				<section>
					<section data-markdown>
						<textarea data-template>
						## Probability Laws

						### Classical Probability

						Generally, a random experiment $\mathrm{E}$ is classical if (i) E contains only different limited basic events, that is, $\Omega={\omega_1, \omega_2, \cdots, \omega_n}$. We call this kind of sample space simple space, and (ii) all outcomes are equally likely to occur.

						If elementary events are assigned equal probabilities, then the probability of a disjunction of elementary events is just the number of events in the disjunction divided by the total number of elementary events. That is, for classical random experiment $\mathrm{E}, \Omega={\omega_1, \omega_2, \cdots, \omega_n}$, we define the probability of event $A$ as
`$$
P(A)=\frac{\# A}{\# \Omega}
$$`
						---
						Example 1.3.4 Suppose that there are $n$ people, each person will be assigned to any of the $N(n \leqslant N)$ rooms with the same probability $1 / N$. We shall determine the probabilities of the following events:
$A$ : For the given $n$ rooms, there is exactly one person in one room.
$B:$ There is exactly one person in one room.
$C:$ There are exactly $m$ people in a given room.
---
Solution. In this experiment, the number of all different possible outcomes are $N^n$.
For event $A$, since $n$ rooms has been given in advance, the first people has $n$ choices, the second people has $n-1$ choices, $\cdots$. So there are $n$ ! possible outcomes in event $A$. We then calculate
$$
P(A)=\frac{n !}{N^n}
$$
For event $B$, the number of the outcomes that $n$ rooms are to be chosen at random from $N$ rooms is $\left(\begin{array}{l}N \\ n\end{array}\right)$. The next step is the same as in event $B$, then
$$
P(B)=\frac{\left(\begin{array}{l}
N \\
n
\end{array}\right) n !}{N^n}
$$
---
For event $C$, the first step is choosing $m$ people from $n$ people at random and let them in the given room, and the number of this process is $\left(\begin{array}{l}n \\ m\end{array}\right)$. The second step is that $n-m$ people are about to be assigned $N-1$ rooms in any way and the number of this process is $(N-1)^{n-m}$. So the probability of event $C$ is
$$
P(C)=\frac{\left(\begin{array}{l}
n \\
m
\end{array}\right)(N-1)^{n-m}}{N^n}
$$
						---
						### Geometric Probability

Problems of the above type and their solution techniques were first studied in the 18 th century, and the general topic became known as geometric probability. A random experiment $\mathrm{E}$ is called to be geometric if
(i) the sample space is a measurable (such as length, area, volume, etc.) region, i.e., $0<$ $L(\Omega)<\infty$, and
(ii) the probability of every event $A \subset \Omega$ is proportional to the measure $L(A)$ and has nothing to do with its position and shape.
In this case, we define the probability of event $A$ as
$$
P(A)=\frac{L(A)}{L(\Omega)}
$$
and $P(\emptyset)=0$
---
Example 1.3.7 (Lunch date problem) You and one of your friends arrange to meet between 12:00 and 13:00. As a result, it is possible for one of you to arrive at random between 12:00 and 13:00 and waits exactly 20 minutes for another one. After 20 minutes, one of you leaves if another person has not arrived. What is the probability that you and your friend will meet?
---
Solution. Let $x$ and $y$ be the time you and your friend arriving the gate, respectively, then our sample space is a square $\Omega=\{(x, y) \mid 12 \leqslant x, y \leqslant 13\}$. Let $M$ be the event that two of you will meet at the gate, then $M$ will occur if and only if $|x-y| \leqslant 1 / 3$, i.e.,
$$
M=\{(x, y)|12 \leqslant x, y \leqslant 13,| x-y \mid \leqslant 1 / 3\}
$$
$M$ is shown as the shaded area in Fig. 1.2. In this problem, the expression "equally likely to occur" means that the probability that the sample point is located in a special region $M \subset \Omega$ is proportional to the area of $M$. Again, since the certain event $\Omega$ has area 1 , the probability of $M$ is equal to the area of $M$, i.e., $P(M)=1-\left(\frac{2}{3}\right)^2=\frac{5}{9}$.
---
![Figure 1.2](https://pic-1307682807.cos.ap-beijing.myqcloud.com/Screenshot%202023-05-31%20at%2017.11.27.png)
						</textarea>
					</section>
				</section>
				<section>
					<section data-markdown>
						<textarea data-template>
							## Probability Space

### Probability Axioms

1. **(Nonnegativity)** $\mathbf{P}(A) \geq 0$, for every event $A$.
2. **(Additivity)** If $A$ and $B$ are two disjoint events, then the probability of their union satisfies
$$
\mathbf{P}(A \cup B)=\mathbf{P}(A)+\mathbf{P}(B) .
$$
More generally, if the sample space has an infinite number of elements and $A_1, A_2, \ldots$ is a sequence of disjoint events, then the probability of their union satisfies
$$
\mathbf{P}\left(A_1 \cup A_2 \cup \cdots\right)=\mathbf{P}\left(A_1\right)+\mathbf{P}\left(A_2\right)+\cdots .
$$
3. **(Normalization)** The probability of the entire sample space $\Omega$ is equal to 1 , that is, $\mathbf{P}(\Omega)=1.$
---
### Properties of Probability Laws
Consider a probability law, and let $A, B$, and $C$ be events.
(a) If $A \subset B$, then $\mathbf{P}(A) \leq \mathbf{P}(B)$, $\mathbf{P}(B-A) = \mathbf{P}(B) - \mathbf{P}(A)$
(b) $\mathbf{P}(A \cup B)=\mathbf{P}(A)+\mathbf{P}(B)-\mathbf{P}(A \cap B)$.
(c) $\mathbf{P}(A \cup B) \leq \mathbf{P}(A)+\mathbf{P}(B)$.
(d) $\mathbf{P}(A \cup B \cup C)=\mathbf{P}(A)+\mathbf{P}\left(A^c \cap B\right)+\mathbf{P}\left(A^c \cap B^c \cap C\right).$
---
Example 1.4.2 Assume $A$ and $B$ are two events such that $P(A)=1 / 3$ and $P(B)=1 / 2$. Determine the value of $P(B-A)$ satisfying each of the following conditions. (a) $A$ and $B$ are disjoint; (b) $A \subset B$; (c) $P(A B)=1 / 8$.
---
Solution. Since $B=\Omega \cap B=(A \cup \bar{A}) B=A B \cup \bar{A} B$, and by Theorem 1.4.2,
$$
P(B)=P(A B)+P(\bar{A} B)=P(A B)+P(B-A)
$$
That is,
$$
P(B-A)=P(B)-P(A B)
$$
(a) Since $A$ and $B$ are disjoint means $A B=\emptyset$, we know
$$
P(B-A)=P(B)-P(A B)=P(B)=1 / 2
$$
(b) If $A \subset B$, then $A B=A$. Thus $P(B-A)=P(B)-P(A B)=P(B)-P(A)=1 / 6.$
(c) $P(B-A)=P(B)-P(A B)=1 / 2-1 / 8=3 / 8$.

						</textarea>
					</section>
				</section>
				<section>
					<section data-markdown>
						<textarea data-template>
							## Conditional Probability

Given two events $A$ and $B$ with $P(B)>0$, the conditional probability of A given $B$ is defined as the quotient of the joint probability of $A$ and $B$, and the probability of $B$ :
$$
P(A \mid B)=\frac{P(A B)}{P(B)}
$$
**Theorem**

If $P(B)>0$, then the conditional probability $P(A \mid B)$ is also a probability, that is,
(i) for every event $A, P(A \mid B) \geqslant 0$;
(ii) $P(\Omega \mid B)=1$
(iii) for every infinite sequence of countable disjoint events $A_1, A_2, \cdots$,
$$
P\left(\bigcup_{i=1}^{\infty} A_i \mid B\right)=\sum_{i=1}^{\infty} P\left(A_i \mid B\right)
$$
---
**Proposition**

If $P(B)>0$, then
(i) $P(\emptyset \mid B)=0$,
(ii) for every finite sequence of countable disjoint events $A_1, A_2, \cdots, A_n$,
$$
P\left(\bigcup_{i=1}^n A_i \mid B\right)=\sum_{i=1}^n P\left(A_i \mid B\right)
$$
(iii) $P(\bar{A} \mid B)=1-P(A \mid B)$,
(iv) if $A \subset C$, then $P(C-A \mid B)=P(C \mid B)-P(A \mid B)$ and $P(A \mid B) \leqslant P(C \mid B)$,
(v) $P(A \cup C \mid B)=P(A \mid B)+P(C \mid B)-P(A C \mid B)$.
---
Example 1.5.4 Ten fair dice are rolled at one time. What is the conditional probability of the event at least two land on 1 given the event at least one of the dice lands on 1.
---
Solution. Let $A$ be the event that at least one of the dice lands on $1, B$ be the event that at least two land on 1 , and $C$ be the event that exactly one of the dice lands on 1 . We see that $B \subset A, C \subset A$ and $B=A-C$. By using the definition of classical probability,
$$
P(A)=1-P(\bar{A})=1-\frac{5^{10}}{6^{10}}, \quad P(C)=\frac{10 \times 5^9}{6^{10}} .
$$
So the required probability is
$$
P(B \mid A)=\frac{P(A B)}{P(A)}=\frac{P(B)}{P(A)}=\frac{P(A)-P(C)}{P(A)}=\frac{1-\frac{5^{10}}{6^{10}}-\frac{10 \times 5^9}{6^{10}}}{1-\frac{5^{10}}{6^{10}}} \approx 0.615
$$
---
### The Multiplication Rule

Suppose that $A_1, A_2, \cdots, A_n$ are events satisfying $P\left(A_1 A_2 \cdots A_{n-1}\right)>0$. Then
$$
P\left(A_1 A_2 \cdots A_n\right)=P\left(A_1\right) P\left(A_2 \mid A_1\right) P\left(A_3 \mid A_1 A_2\right) \cdots P\left(A_n \mid A_1 A_2 \cdots A_{n-1}\right) .
$$
Proof. The product of probabilities on the right side of equation is equal to
$$
P\left(A_1\right) \cdot \frac{P\left(A_1 A_2\right)}{P\left(A_1\right)} \cdot \frac{P\left(A_1 A_2 A_3\right)}{P\left(A_1 A_2\right)} \cdots \frac{P\left(A_1 A_2 \cdots A_n\right)}{P\left(A_1 A_2 \cdots A_{n-1}\right)}
$$
---
Example 1.5.8 (Polya urn mode) Suppose that an urn contains $r$ red balls and $b$ blue balls $(r \geqslant 2, b \geqslant 2)$. Suppose that one ball is drawn randomly from the urn and its color is observed; it is then replaced in the urn, and $c$ additional balls of the same color is added to the urn, and the selection process is repeated four times. We shall determine the probability of obtaining the sequence of outcomes red, blue, red, blue.

Solution. Let $R_j$ denote the event that a red ball is obtained on the $j$ th draw and $B_j$ the event that a blue ball is obtained on the $j$ th draw $(j=1,2,3,4)$. Then
`$$
\begin{aligned}
P\left(R_1 B_2 R_3 B_4\right) & =P\left(R_1\right) P\left(B_2 \mid R_1\right) P\left(R_3 \mid R_1 B_2\right) P\left(B_4 \mid R_1 B_2 R_3\right) \\
& =\frac{r}{r+b} \cdot \frac{b}{r+b+c} \cdot \frac{r+c}{r+b+2 c} \cdot \frac{b+c}{r+b+3 c}
\end{aligned}
$$`
---
### Total Probability Formula

Let $A_1, \ldots, A_n$ be disjoint events that form a partition of the sample space (each possible outcome is included in exactly one of the events $A_1, \ldots, A_n$ ) and assume that $\mathbf{P}\left(A_i\right)>0$, for all $i$. Then, for any event $B$, we have

`$$
\begin{aligned}
\mathbf{P}(B) & =\mathbf{P}\left(A_1 \cap B\right)+\cdots+\mathbf{P}\left(A_n \cap B\right) \\
& =\mathbf{P}\left(A_1\right) \mathbf{P}\left(B \mid A_1\right)+\cdots+\mathbf{P}\left(A_n\right) \mathbf{P}\left(B \mid A_n\right)
\end{aligned}
$$`


Example 1.5.9 Two cards from an ordinary deck of 52 cards are missing. What is the probability that a random card drawn from this deck is a spade?
---
Solution. Let $A$ be the event that the randomly drawn card is a spade. Let $B_i$ be the event that $i$ spades are missing from the 50-card (defective) deck, for $i=0,1,2$.
By conditioning on how many spades are missing from the original (good) deck, we get
`$$
\begin{aligned}
P(A) & =P\left(B_0\right) P\left(A \mid B_0\right)+P\left(B_1\right) P\left(A \mid B_1\right)+P\left(B_2\right) P\left(A \mid B_2\right) \\
& =\frac{13}{50} \frac{\left(\begin{array}{c}
13 \\
0
\end{array}\right)\left(\begin{array}{c}
39 \\
2
\end{array}\right)}{\left(\begin{array}{c}
52 \\
2
\end{array}\right)}+\frac{12}{50} \frac{\left(\begin{array}{c}
13 \\
1
\end{array}\right)\left(\begin{array}{c}
39 \\
1
\end{array}\right)}{\left(\begin{array}{c}
52 \\
2
\end{array}\right)}+\frac{11}{50} \frac{\left(\begin{array}{c}
13 \\
2
\end{array}\right)\left(\begin{array}{c}
39 \\
0
\end{array}\right)}{\left(\begin{array}{c}
52 \\
2
\end{array}\right)} \approx \frac{1}{4} .
\end{aligned}
$$`
---
### Bayes' Rule

Let $A_1, A_2, \ldots, A_n$ be disjoint events that form a partition of the sample space, and assume that $\mathbf{P}\left(A_i\right)>0$, for all $i$. Then, for any event $B$ such that $\mathbf{P}(B)>0$, we have
`$$
\begin{aligned}
\mathbf{P}\left(A_i \mid B\right) & =\frac{\mathbf{P}\left(A_i\right) \mathbf{P}\left(B \mid A_i\right)}{\mathbf{P}(B)} \\
& =\frac{\mathbf{P}\left(A_i\right) \mathbf{P}\left(B \mid A_i\right)}{\mathbf{P}\left(A_1\right) \mathbf{P}\left(B \mid A_1\right)+\cdots+\mathbf{P}\left(A_{\boldsymbol{n}}\right) \mathbf{P}\left(B \mid A_{\boldsymbol{n}}\right)}
\end{aligned}
$$`
---
Example 1.5.13 Three different machines $M_1, M_2$ and $M_3$ were used for producing a large batch of similar manufactured items. Suppose that 20 percent of the items were produced by machine $M_1, 30$ percent of the items were produced by machine $M_2$, and 50 percent of the items were produced by machine $M_3$. Suppose further that 1 percent of the items produced by machine $M_1$ are defective, that 2 percent of the items produced by machine $M_2$ are defective, and that 3 percent of the items produced by machine $M_3$ are defective. Finally, suppose that one item is selected at random from the entire batch, and it is found to be defective. We shall determine the probability that this item was produced by machine $M_i(i=1,2,3)$.
---
Solution. Let $B_i$ be the event that the selected item was produced by machine $M_i(i=1,2,3)$, and let $A$ be the event that the selected item is defective. We must evaluate the conditinal probability $P\left(B_i \mid A\right)$. The probability $P\left(B_i\right)(i=1,2,3)$ is as follows:
$$
P\left(B_1\right)=0.2, \quad P\left(B_2\right)=0.3, \quad P\left(B_3\right)=0.5
$$
Furthermore, the probability $P\left(A \mid B_i\right)$ that an item produced by machine $M_i$ will be defective is: $P\left(A \mid B_1\right)=0.01, P\left(A \mid B_2\right)=0.02, P\left(A \mid B_3\right)=0.03$. It now follows from Bayes' theorem that
`$$
\begin{aligned}
P\left(B_1 \mid A\right) & =\frac{P\left(B_1\right) P\left(A \mid B_1\right)}{P\left(B_1\right) P\left(A \mid B_1\right)+P\left(B_2\right) P\left(A \mid B_2\right)+P\left(B_3\right) P\left(A \mid B_3\right)} \\
& =\frac{(0.2)(0.01)}{(0.2)(0.01)+(0.3)(0.02)+(0.5)(0.03)}=0.087 .
\end{aligned}
$$`
By the similar way, we obtain $P\left(B_2 \mid A\right)=0.261, P\left(B_3 \mid A\right)=0.652$.

						</textarea>
					</section>
				</section>
				
				<section>
					<section data-markdown>
						<textarea data-template>
							## Independence

Two events $A$ and $B$ are said to be independent if
$$
\mathbf{P}(A \cap B)=\mathbf{P}(A) \mathbf{P}(B) .
$$
It is obvious that if $P(A)>0$ and $P(B)>0$, then independence is equivalent to the statement that the conditional probability of one event given the other is the same as the unconditional probability of the event:
$$
P(A \mid B)=P(A) \Longleftrightarrow P(B \mid A)=P(B) \Longleftrightarrow P(A B)=P(A) P(B)
$$
**Theorem** Suppose that $A$ and $B$ are disjoint events for an experiment, each with positive probability. Then $A$ and $B$ are dependent.

**Theorem** If $A$ and $B$ are independent events in an experiment, then each of the following pairs of events is independent: (i) $\bar{A}$ and $B$; (ii) $A$ and $\bar{B}$; (iii) $\bar{A}$ and $\bar{B}$.
---
### Conditional Independence

Two events $A$ and $B$ are said to be conditionally independent, given another event $C$ with $\mathbf{P}(C)>0$, if
$$
\mathbf{P}(A \cap B \mid C)=\mathbf{P}(A \mid C) \mathbf{P}(B \mid C)
$$
If in addition, $\mathbf{P}(B \cap C)>0$, conditional independence is equivalent to the condition
$$
\mathbf{P}(A \mid B \cap C)=\mathbf{P}(A \mid C) .
$$
Independence does not imply conditional independence, and vice versa.
---
### Independence of Several Events
We say that the events $A_1, A_2, \ldots, A_n$ are independent if
$$
\mathbf{P}\left(\bigcap_{i \in S} A_i\right)=\prod_{i \in S} \mathbf{P}\left(A_i\right), \quad \text { for every subset } S \text { of }\{1,2, \ldots, n\}
$$
For the case of three events, $A_1, A_2$, and $A_3$, independence amounts to satisfying the four conditions
`$$
\begin{aligned}
\mathbf{P}\left(A_1 \cap A_2\right) & =\mathbf{P}\left(A_1\right) \mathbf{P}\left(A_2\right), \\
\mathbf{P}\left(A_1 \cap A_3\right) & =\mathbf{P}\left(A_1\right) \mathbf{P}\left(A_3\right), \\
\mathbf{P}\left(A_2 \cap A_3\right) & =\mathbf{P}\left(A_2\right) \mathbf{P}\left(A_3\right), \\
\mathbf{P}\left(A_1 \cap A_2 \cap A_3\right) & =\mathbf{P}\left(A_1\right) \mathbf{P}\left(A_2\right) \mathbf{P}\left(A_3\right) .
\end{aligned}
$$`
The first three conditions simply assert that any two events are independent, a property known as **pairwise independence**（部分独立）. But the fourth condition is also important and does not follow from the first three. Conversely, the fourth condition does not imply the first three.
---
### Independent Trials and the Binomial Probabilities

If an experiment involves a sequence of independent but identical stages, we say that we have a sequence of **independent trials** . In the special case where there are only two possible results at each stage, we say that we have a sequence of independent **Bernoulli trials**.

Let us now consider the probability
`$$
p(k)=\mathbf{P}(k \text { heads come up in an } n \text {-toss sequence }),
$$`
which will play an important role later. We showed above that the probability of any given sequence that contains $k$ heads is $p^k(1-p)^{n-k}$, so we have
`$$
p(k)=\left(\begin{array}{l}
n \\
k
\end{array}\right) p^k(1-p)^{n-k}
$$`
where we use the notation
$\left(\begin{array}{l}n \\ k\end{array}\right)=$ number of distinct $n$-toss sequences that contain $k$ heads.
The numbers $\left(\begin{array}{l}n \\ k\end{array}\right)$ (read as " $n$ choose $k$ ") are known as the **binomial coeffcients**, while the probabilities $p(k)$ are known as the **binomial probabilities**. 
---
Using a counting argument, we can show that
`$$
\left(\begin{array}{l}
n \\
k
\end{array}\right)=\frac{n !}{k !(n-k) !}, \quad k=0,1, \ldots, n
$$`
Note that the binomial probabilities $p(k)$ must add to 1, thus showing the binomial formula
`$$
\sum_{k=0}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) p^k(1-p)^{n-k}=1
$$`
						</textarea>
					</section>
				</section>
				
				<section>
					<h1 style="color: #2196f3;">Random Variable</h1>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## Cumulative Distribution Functions

We have been dealing with discrete and continuous random variables in a somewhat different manner. using PMFs and PDFs, respectively. It would be desirable to describe all kinds of random variables with a single mathematical concept. This is accomplished with the cumulative distribution function, or CDF for short. The CDF of a random variable $X$ is denoted by $F_X$ and provides the probability $\mathbf{P}(X \leq x)$. In particular, for every $x$ we have
`$$
F_{\boldsymbol{X}}(x)=\mathbf{P}(X \leq x)= \begin{cases}\sum_{k \leq x} p_X(k), & \text { if } X \text { is discrete, } \\ \int_{-\infty}^x f_X(t) d t, & \text { if } X \text { is continuous. }\end{cases}
$$`
Loosely speaking, the CDF $F_X(x)$ "accumulates" probability "up to" the value $x$.
---
### Properties of a CDF
The CDF $F_X$ of a random variable $X$ is defined by
`$$
F_X(x)=\mathbf{P}(X \leq x), \quad \text { for all } x
$$`
and has the following properties.
- $F_X$ is monotonically nondecreasing:
if $x \leq y$, then $F_X(x) \leq F_X(y)$
- $F_X(x)$ tends to 0 as $x \rightarrow-\infty$, and to 1 as $x \rightarrow \infty$.
- It is always right-continuous; that is, $F(x)=F\left(x^{+}\right)$at every point $x$.
- If $X$ is discrete, then $F_X(x)$ is a piecewise constant function of $x$.
- If $X$ is continuous, then $F_X(x)$ is a continuous function of $x$.
- If $X$ is discrete and takes integer values, the PMF and the CDF can be obtained from each other by summing or differencing:
`$$
\begin{gathered}
F_X(k)=\sum_{i=-\infty}^k p_X(i) \\
p_X(k)=\mathbf{P}(X \leq k)-\mathbf{P}(X \leq k-1)=F_X(k)-F_X(k-1),
\end{gathered}
$$`
---
for all integers $\boldsymbol{k}$.
- If $X$ is continuous, the PDF and the CDF can be obtained from each other by integration or differentiation:
`$$
F_X(x)=\int_{-\infty}^x f_X(t) d t, \quad f_X(x)=\frac{d F_X}{d x}(x)
$$`
(The second equality is valid for those $x$ at which the PDF is continuous.)
---
**Theorem**

- For every value $x, P(X>x)=1-F(x)$.
- For all values $x_1$ and $x_2$ such that $x_1<x_2$,

`$$
P\left(x_1<X \leqslant x_2\right)=F\left(x_2\right)-F\left(x_1\right)
$$`
- For each value $x, P(X<x)=F\left(x^{-}\right)$.
---
**Corollary**

For every value $x$,
`$$
P(X=x)=F(x)-F\left(x^{-}\right) \quad \text { and } \quad P(X \geqslant x)=1-F\left(x^{-}\right) \text {. }
$$`
For all values $x_1$ and $x_2$ such that $x_1<x_2$,
`$$
\begin{aligned}
& P\left(x_1<X<x_2\right)=F\left(x_2^{-}\right)-F\left(x_1\right), \\
& P\left(x_1 \leqslant X<x_2\right)=F\left(x_2^{-}\right)-F\left(x_1^{-}\right),
\end{aligned}
$$`
and
`$$
P\left(x_1 \leqslant X \leqslant x_2\right)=F\left(x_2\right)-F\left(x_1^{-}\right) .
$$`
---
Example 2.2.4 Suppose that the d.f. $F$ of a random variable $X$ is as sketched in the Fig. 2.4. Find each of the following probabilities:
(a) $P(X=-1)$
(b) $P(X<0)$
(c) $P(X \leqslant 0)$
(d) $P(X=1)$
(e) $P(0<X \leqslant 3)$
(f) $P(0<X<3)$
(g) $P(0 \leqslant X \leqslant 3)$
(h) $P(1<X \leqslant 2)$
(i) $P(1 \leqslant X \leqslant 2)$
(j) $P(X>5)$
(k) $P(X \geqslant 5)$
(l) $P(3 \leqslant X \leqslant 4)$

Solution.
(a) $P(X=-1)=F(-1)-F\left(-1^{-}\right)=0.1-0=0.1$,
(b) $P(X<0)=F\left(0^{-}\right)=0.1$,
(c) $P(X \leqslant 0)=F(0)=0.2$,
(d) $P(X=1)=F(1)-F\left(1^{-}\right)=0.3-0.3=0$,
(e) $P(0<X \leqslant 3)=F(3)-F(0)=0.8-0.2=0.6$,
(f) $P(0<X<3)=F\left(3^{-}\right)-F(0)=0.6-0.2=0.4$,
(g) $P(0 \leqslant X \leqslant 3)=F(3)-F\left(0^{-}\right)=0.8-0.1=0.7$,
(h) $P(1<X \leqslant 2)=F(2)-F(1)=0.3-0.3=0$,
(i) $P(1 \leqslant X \leqslant 2)=F(2)-F\left(1^{-}\right)=0.3-0.3=0$,
(j) $P(X>5)=1-P(X \leqslant 5)=1-F(5)=1-1=0$,
(k) $P(X \geqslant 5)=1-F\left(5^{-}\right)=1-1=0$,
(l) $P(3 \leqslant X \leqslant 4)=F(4)-F\left(3^{-}\right)=0.8-0.6=0.2$.
						</textarea>
					</section>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## The Distribution Function of Function of a Random Variable

Let $X$ be a continuous random variable having p.d.f. $f_X$. Suppose that $g(x)$ is a strictly monotonic (increasing or decreasing), differentiable (and thus continuous) function of $x$. Then the random variable $Y$ defined by $Y=g(X)$ has a p.d.f. given by
$$
f_Y(y)= \begin{cases}f_X\left(g^{-1}(y)\right)\left|\frac{d}{d y} g^{-1}(y)\right| & \text { if } y=g(x) \text { for some } x \\ 0 & \text { if } y \neq g(x) \text { for all } x\end{cases}
$$
where $x=g^{-1}(y)$ is defined to equal that value of $x$ such that $g(x)=y$.
---
Proof. We prove Theorem when $g(x)$ is an increasing function. Suppose that $y=g(x)$ for some $x$. Then, with $Y=g(X)$,
$$
F_Y(y)=P(g(X) \leqslant y)=P\left(X \leqslant g^{-1}(y)\right)=F_X\left(g^{-1}(y)\right) .
$$
Differentiation gives
$$
f_Y(y)=f_X\left(g^{-1}(y)\right) \frac{d}{d y} g^{-1}(y)
$$
which agrees with the theorem, since $g^{-1}(y)$ is nondecreasing, so its derivative is nonnegative.
When $y \neq g(x)$ for any $x$, then $F_Y(y)$ is either 0 or 1 , and in either case $f_Y(y)=0$.
---
Let
`$$
f(x)= \begin{cases}c & \text { for }-2 \leqslant x \leqslant-1 \\ 2 c & \text { for }-1<x \leqslant 1 \\ 0 & \text { otherwise }\end{cases}
$$`
be the density of a continuously  distributed random variable $X$. Compute the density of $Y=X^2$.
---
Solution.
`$$
\begin{aligned}
& F(x)=\int_{-\infty}^{+\infty} f(x) d x=\int_{-\infty}^x f(t) d t \\
& =\left\{\begin{array}{l}
c x+2 c,-2 \leq x \leq-1 \\
2 c x+3 c,-1<x \leq 1 \\
5 c, x>1
\end{array}\right. \\
& \because F(+\infty)=1 \therefore \quad 5 c=1, \quad c=\frac{1}{5} \\
& E_Y(y)=P(Y \leq y)=P\left(x^2 \leq y\right) \\
& =P(-\sqrt{y} \leq x \leq \sqrt{y}) \\
& =F(\sqrt{y})-F(-\sqrt{y}) \\
& =\left\{\begin{array}{l}
\frac{4}{5} \sqrt{y}, 0 \leq y \leq 1 \\
\frac{1}{5} \sqrt{y}+\frac{3}{5}, y>1
\end{array}\right. \\
& \therefore f_Y(y)=\left\{\begin{array}{cc}
\frac{2}{5} y^{-\frac{1}{2}}, & 0 \leq y \leq 1 \\
\frac{1}{10} y^{-\frac{1}{2}}, & y>1 \\
0, & \text { otherwis. }
\end{array}\right. \\
&
\end{aligned}
$$`
						</textarea>
					</section>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## Expectation and Variance
We define the **expected value** (also called the **expectation** or the **mean**) of a random variable $X$, with PMF $p_X$, by
`$$
\mathbf{E}[X]=\sum_x x p_X(x)
$$`
---
### Expected Value Rule for Functions of Random Variables
Let $X$ be a random variable with PMF $p_X$, and let $g(X)$ be a function of $X$. Then, the expected value of the random variable $g(X)$ is given by
`$$
\mathbf{E}[g(X)]=\sum_x g(x) p_X(x)
$$`
---
### Variance

The most important quantity associated with a random variable $X$, other than the mean, is its **variance**, which is denoted by $\operatorname{var}(X)$ and is defined as the expected value of the random variable $(X-\mathbf{E}[X])^2$, i.e.,
$$
\operatorname{var}(X)=\mathbf{E}\left[(X-\mathbf{E}[X])^2\right] .
$$
and can be calculated as
$$
\operatorname{var}(X)=\sum_x(x-\mathbf{E}[X])^2 p_X(x)
$$
Since $(X-\mathrm{E}[X])^2$ can only take nonnegative values, the variance is always nonnegative.

The variance provides a measure of dispersion of $X$ around its mean. Another measure of dispersion is the **standard deviation** of $X$, which is defined as the square root of the variance and is denoted by $\sigma_X$ :
$$
\sigma_X=\sqrt{\operatorname{var}(X)}
$$
The standard deviation is often easier to interpret because it has the same units as $X$. For example, if $X$ measures length in meters, the units of variance are square meters, while the units of the standard deviation are meters.
---
### Variance in Terms of Moments Expression

$$
\operatorname{var}(X)=\mathbf{E}\left[X^2\right]-(\mathbf{E}[X])^2
$$
---
### Mean and Variance of a Linear Function of a Random Variable

Let $X$ be a random variable and let
$$
Y=a X+b .
$$
where $a$ and $b$ are given scalars. Then,
$$
\mathbf{E}[Y]=a \mathbf{E}[X]+b, \quad \operatorname{var}(Y)=a^2 \operatorname{var}(X)
$$

---
2.27 Each game you play is a win with probability $p$. You plan to play 5 games, but if you win the fifth game, then you will keep on playing until you lose.
(a) Find the expected number of games that you play.
(b) Find the expected number of games that you lose.
---
solution.(a). We defimen Random varible $X$ is about extra number of games that I play We can know are least we can play 5 times
`$$
\begin{aligned}
& \therefore p(x=x)=p^n(1-p) . \quad x=0,1,2, \cdots \\
& \therefore E(x)=\sum_{n=0}^{\infty} n \cdot p^n(1-p)\\&=\operatorname{lim}_{n \rightarrow \infty}\left[p(1-p)+2 p^2(1-p)+\cdots n p^n(1-p)\right]\\&=\frac{p}{1-p}
\end{aligned}
$$`
the expected number of games is $\left(5+\frac{p}{1-p}\right)$
---
(a) when I didn't win the fifth game. (probability is $1-p$), I play 5 games.
When I win the fifth game (probability is $p$ ). after the fifth game, it is a geometric distribution. Then the expected number is $5+\frac{1}{1-p}$
`$$
\begin{aligned}
E(x) & =5 \cdot(1-p)+\left(5+\frac{1}{1-p}\right) \cdot p \\
& =5+\frac{p}{1-p}
\end{aligned}
$$`
---
(b). Define a random variable $Y$ is about the game which loses before we stare the fifth game
`$$
\begin{aligned}
& P(Y=y)=C_4^y p^{4-y}(1-p)^y \quad y=0,1,2,3,4 \\
& Y \sim B(4,(1-p)) \quad \therefore E(Y)=4(1-p)
\end{aligned}
$$`
Since after the fifth game, we can only lose 1 game, the expected number of lost games is $(4(1-p)+1)$
						</textarea>
					</section>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## Discrete Random Variables
							### The Binomial Random Variable

We refer to $X$ as a **binomial** random variable **with parameters $n$ and $p$**. The PMF of $X$ consists of the binomial probabilities that were calculated in Section 1.5:
`$$
p_X(k)=\mathbf{P}(X=k)=\left(\begin{array}{l}
n \\
k
\end{array}\right) p^k(1-p)^{n-k}, \quad k=0,1, \ldots, n
$$`
(Note that here and elsewhere, we simplify notation and use $k$, instead of $x$, to denote the values of integer-valued random variables.) The normalization property, specialized to the binomial random variable, is written as
`$$
\sum_{k=0}^n\left(\begin{array}{l}
n \\
k
\end{array}\right) p^k(1-p)^{n-k}=1
$$`
---
### ![截屏2023-01-20 18.05.57](https://pic-1307682807.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2023-01-20%2018.05.57.png)
---
### Poisson limit theorem

Suppose that $\lambda$ is a constant and $n$ is a positive integer. If $\lim _{n \rightarrow \infty} n p_n=\lambda$, then we have
`$$
\lim _{n \rightarrow+\infty}\left(\begin{array}{l}
n \\
x
\end{array}\right) p_n^x\left(1-p_n\right)^{n-x}=e^{-\lambda} \frac{\lambda^x}{x !}
$$`
for any fixed nonnegative integer $x$.
---
2.43 Based upon past experience, 1 % of the telephone bills mailed to households are incorrect. If a sample of 20 bills is selected, using the binomial distribution and the Poisson approximation to the binomial distribution, find the probability that at least one bill is incorrect. Briefly compare and explain your results.
---
Solution. Let x=# bill (s) incorrect , $\lambda=n p=20 \times 0.01=0.2$ 

binomial distribution
`$$
\begin{aligned}
P_b(x \geqslant 1)=1-P(x=0) & =1-0.99^{20} \\
& \approx 0.1821
\end{aligned}
$$`
Poisson approximation
`$$
\begin{aligned}
& P_p(x \geqslant 1)=1-P(x=0)=1-(0.99)^{20} \\
& \approx 1-e^{-\frac{1}{5}} \\
& \approx 0.18 B \\
& P_b(x \geqslant 1) \approx P_p(x \geqslant 1)
\end{aligned}
$$`
---
### The Geometric Random Variable

Suppose that we repeatedly and independently toss a coin with probability of a head equal to $p$, where $0<p<1$. The **geometric** random variable is the number $X$ of tosses needed for a head to come up for the first time. Its PMF is given by
$$
p_X(k)=(1-p)^{k-1} p, \quad k=1,2, \ldots,
$$
since $(1-p)^{k-1} p$ is the probability of the sequence consisting of $k-1$ successive tails followed by a head. This is a legitimate PMF because
$$
\sum_{k=1}^{\infty} p_X(k)=\sum_{k=1}^{\infty}(1-p)^{k-1} p=p \sum_{k=0}^{\infty}(1-p)^k=p \cdot \frac{1}{1-(1-p)}=1
$$
---
##### ![截屏2023-01-20 18.06.21](https://pic-1307682807.cos.ap-beijing.myqcloud.com/%E6%88%AA%E5%B1%8F2023-01-20%2018.06.21.png)
---
The distribution function of $X$ is given by
$$
F(x)=P(X \leqslant x)=\sum_{k=1}^x p(k)=p \sum_{k=1}^x q^{k-1}=p \frac{1-q^x}{1-q}=1-q^x
$$
for $x=1,2, \cdots$. Then we deduce that $P(X>x)=q^x$. Making use of the formula, we can show that
$$
P(X>x+y \mid X>x)=P(X>y)
$$
for any $x, y \in 0,1,2, \cdots$. This property is known as the **memoryless** property of the geometric distribution.
						</textarea>
					</section>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## Continuous Random Variables
							### Exponential Random Variable

An exponential random variable has a PDF of the form
`$$
f_X(x)= \begin{cases}\lambda e^{-\lambda x}, & \text { if } x \geq 0, \\ 0, & \text { otherwise }\end{cases}
$$`
where $\lambda$ is a positive parameter characterizing the PDF. This is a legitimate PDF because
`$$
\int_{-\infty}^{\infty} f_X(x) d x=\int_0^{\infty} \lambda e^{-\lambda x} d x=-\left.e^{-\lambda x}\right|_0 ^{\infty}=1
$$`
---
Note that the probability that $X$ exceeds a certain value decreases exponentially. Indeed, for any $a \geq 0$, we have
`$$
\mathbf{P}(X \geq a)=\int_a^{\infty} \lambda e^{-\lambda x} d x=-\left.e^{-\lambda x}\right|_a ^{\infty}=e^{-\lambda a}
$$`
The mean and the variance can be calculated to be
`$$
\mathbf{E}[X]=\frac{1}{\lambda}, \quad \operatorname{var}(X)=\frac{1}{\lambda^2}
$$`
---
We can show that the exponential distribution, just as the geometric distribution, possesses the memoryless property. That is, if $X \sim \operatorname{Exp}(\lambda)$, then
`$$
P(X>t+s \mid X>t)=P(X>s) \quad \text { for } s, t \geqslant 0 .
$$`
Example 2.5.6 The lifetime (in years) of a radio has an exponential distribution with parameter $\lambda=1 / 10$. If we buy a five-year-old radio, what is the probability that it will work for less than 10 additional years?
---
Solution. Let $X$ be the total lifetime of the radio. We have that $X \sim \operatorname{Exp}(\lambda=1 / 10)$. We seek
`$$
\begin{aligned}
P(X \leqslant 15 \mid X>5) & =1-P(X>15 \mid X>5)=1-P(X>10)=P(X \leqslant 10) \\
& =\int_0^{10} \frac{1}{10} e^{-x / 10} d x=-\left.e^{-x / 10}\right|_0 ^{10}=1-e^{-1} \approx 0.6321 .
\end{aligned}
$$`
---
### Normal Random Variables

A continuous random variable $X$ is said to be **normal** or **Gaussian** if it has a PDF of the form (see Fig. 3.9)
`$$
f_X(x)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-(x-\mu)^2 / 2 \sigma^2},
$$`
where $\mu$ and $\sigma$ are two scalar parameters characterizing the PDF. with $\sigma$ assumed positive. It can be verified that the normalization property
`$$
\frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} e^{-(x-\mu)^2 / 2 \sigma^2} d x=1
$$`
holds (see the end-of-chapter problems).
---
### The Standard Normal Random Variable

A normal random variable $Y$ with zero mean and unit variance is said to be a standard normal. Its CDF is denoted by $\Phi$ :
`$$
\Phi(y)=\mathbf{P}(Y \leq y)=\mathbf{P}(Y<y)=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^y e^{-t^2 / 2} d t .
$$`
It is recorded in a table, and is a very useful tool for calculating various probabilities involving normal random variables.

2.56 Evaluate the integral $\int_0^{+\infty} e^{-4 x^2} d x$.
---
Solution. we know from stanard normal distribution: $\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} \cdot e^{-\frac{x^2}{2}} d x=1$
`$$
\Rightarrow \int_{-\infty}^{+\infty} e^{-\frac{x^2}{2}} d x=\sqrt{2 \pi}
$$`
and $e^{-\frac{x^2}{2}}$ is even
`$$
\Rightarrow \int_0^{+\infty} e^{-\frac{x^2}{2}} d x=\frac{1}{2} \int_{-\infty}^{\infty} e^{-\frac{x^2}{2}} d x=\frac{\sqrt{2 \pi}}{2}
$$`
so $\int_0^{+\infty} e^{-4 x^2} d x=\frac{1}{2 \sqrt{2}} \int_0^{+\infty} e^{-\frac{(2 \sqrt{2} x)^2}{2}} d(2 \sqrt{2} x)$
`$$
\begin{aligned}
& t=2 \sqrt{2} \times \frac{1}{2 \sqrt{2}} \int_0^{+\infty} e^{-\frac{t^2}{2}} d t \\
&=\frac{1}{2 \sqrt{2}} \times \frac{\sqrt{2 \pi}}{2} \\
&=\frac{\sqrt{\pi}}{4}
\end{aligned}
$$`
---
![Screenshot 2023-06-09 at 17.28.41](https://pic-1307682807.cos.ap-beijing.myqcloud.com/Screenshot%202023-06-09%20at%2017.28.41.png)
						</textarea>
					</section>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## Random Vectors and Joint Distributions

The joint distribution function (joint d.f.) of $X$ and $Y$ is defined by

$$
F(x, y)=P(X \leqslant x, Y \leqslant y), \quad-\infty<x, y<\infty
$$

We can say that $F(x, y)$ is the probability $(X, Y) \in D_{1}$ in Fig. 3.1(a).

(i) for any $x_{1}<x_{2}, y_{1}<y_{2}$, the event $\left\{X \leqslant x, y_{1}<Y \leqslant y_{2}\right\}$ consists of all points $(x, y)$ in the horizontal half-strip $D_{2}$ of Fig. 3.1(b), we maintain that

$$
P\left(X \leqslant x, y_{1}<Y \leqslant y_{2}\right)=F\left(x, y_{2}\right)-F\left(x, y_{1}\right)
$$
---
(ii) for the event $\left\{x_{1}<X \leqslant x_{2}, Y \leqslant y\right\}$ consists of all points $(x, y)$ in the vertical half-strip $D_{3}$ of Fig. 3.1(b), we maintain that

$$
P\left(x_{1}<X \leqslant x_{2}, Y \leqslant y\right)=F\left(x_{2}, y\right)-F\left(x_{1}, y\right)
$$

(iii) for the event $\left\{x_{1}<X \leqslant x_{2}, y_{1}<Y \leqslant y_{2}\right\}$ consists of all points $(x, y)$ in the rectangle $D_{4}$ of Fig. 3.1(c), we have

$$
P\left(x_{1}<X \leqslant x_{2}, y_{1}<Y \leqslant y_{2}\right)=F\left(x_{2}, y_{2}\right)-F\left(x_{1}, y_{2}\right)-F\left(x_{2}, y_{1}\right)+F\left(x_{1}, y_{1}\right) \text {. }
$$
---
Proposition 3.1.1 The joint distribution function $F(x, y)$ is such that

(i) for any $x_{1}<x_{2}, F\left(x_{1}, y\right) \leqslant F\left(x_{2}, y\right)$; for any $y_{1}<y_{2}, F\left(x, y_{1}\right) \leqslant F\left(x, y_{2}\right).$

(ii) $F(-\infty, y)=0, \quad F(x,-\infty)=0, \quad F(\infty, \infty)=1.$

(iii) $\lim _{x \rightarrow x_{0}^{+}} F(x, y)=F\left(x_{0}, y\right), \quad \lim _{y \rightarrow y_{0}^{+}} F(x, y)=F\left(x, y_{0}\right)$.

 $F(x,+\infty)$ and $F(+\infty, y)$ are the marginal distributions of $X$ and $Y$.
`$$
\begin{aligned}
P(X>x, Y>y) & =1-P(\overline{\{X>x, Y>y\}}) \\
& =1-p(\overline{\{X>x\}} \cup \overline{\{Y>y\}}) \\
& =1-P(\{X \leqslant x\} \cup\{Y \leqslant y\}) \\
& =1-[P(X \leqslant x)+P(Y \leqslant y)-P(X \leqslant x, Y \leqslant y)] \\
& =1-F_{X}(x)-F_{Y}(y)+F(x, y) .
\end{aligned}
$$`
---
### Discrete Random Vectors

For discrete bivariate random vectors, we always use the joint probability function (joint p.f.)

$$
p(x, y):=P(\{X=x\} \cap\{Y=y\}) \equiv P(X=x, Y=y)
$$

of the pair of discrete random vector $(X, Y)$, whose possible values are a (finite or countably infinite) set of points $\left(x_{j}, y_{k}\right)$ in the plane, where $p(x, y)$ has the following properties:

(i) $p\left(x_{j}, y_{k}\right) \geqslant 0, \forall\left(x_{j}, y_{k}\right) \in \mathbb{R} \times \mathbb{R}$;

(ii) $\sum_{j=1}^{\infty} \sum_{k=1}^{\infty} p\left(x_{j}, y_{k}\right)=1$.
---
3.9 The random variables $U$ and $V$ each take the values \pm 1 . Their joint distribution is given by

`$$
\begin{gathered}
P(U=1)=P(U=-1)=\frac{1}{2} . \\
P(V=1 \mid U=1)=\frac{1}{3}=P(V=-1 \mid U=-1), \\
P(V=-1 \mid U=1)=\frac{2}{3}=P(V=1 \mid U=-1) .
\end{gathered}
$$`

(a) Find the probability that $x^{2}+U x+V=0$ has at least one real root.

(b) Find the expected value of the larger root given that there is at least one real root.

(c) Find the probability that $x^{2}+(U+V) x+U+V=0$ has at least one real root.
---
Solution. 
`$$
\begin{aligned}
& P(U=1, V=1)=P(V=1 \mid U=1) \cdot P(U=1) \\
&=\frac{1}{3} \times \frac{1}{2}=\frac{1}{6} \\
& P(U=1, V=-1)=P(V=-1 \mid U=1) \cdot P(U=1) \\
&=\frac{2}{3} \times \frac{1}{2}=\frac{1}{3} \\
& P(U=-1, V=1)=P(V=1 \mid U=-1) \cdot P(U=-1) \\
&=\frac{2}{3} \times \frac{1}{2}=\frac{1}{3} \\
& P(U=-1, V=-1)=P(V=-1 \mid(U=-1) \cdot P(U=-1) \\
&=\frac{1}{3} \times \frac{1}{2}=\frac{1}{6}
\end{aligned}
$$`
---
(a) Since $\Delta=U^2-4 V \geqslant 0$, Then
`$$
\begin{aligned}
P(U^2-4 V \geqslant 0)& =P(U^2 \geq 4 V)\\
& =P(U=1, V=-1)+P(U=-1, V=-1) \\
& =\frac{1}{2}
\end{aligned}
$$`
---
(b) When $U=1, V=-1$
`$$
x^2+U x+V=0 \Leftrightarrow x^2+x-1=0
$$`
Then $x_1=\frac{\sqrt{5}-1}{2} x_2: \frac{-\sqrt{5}-1}{2}$
when $u=1, v=-1$
`$$
x^2+U x+V=0 \Leftrightarrow x^2-x-1: 0
$$`
Then $x_1=\frac{\sqrt{5}+1}{2} x_2=\frac{-\sqrt{5}+1}{2}$
Then 
`$$
\begin{align}
E\left(x_{\text {large }} \mid \Delta \geqslant 0\right)&=\frac{\frac{\sqrt{5}-1}{2} \times \frac{1}{3}+\frac{\sqrt{5}+1}{2} \times \frac{1}{6}}{\frac{1}{2}}\\&=\frac{3 \sqrt{5-1}}{6}\end{align}
$$`
---
(C) Since
`$$
\begin{aligned}
\Delta & =(u+v)^2-4(u+v) \\
& =(u+v-c)(u+v) \geqslant 0
\end{aligned}
$$`
Then
`$$
\begin{aligned}
P(\Delta \geqslant 0) & =P(u=1, v=-1)+P(u=-1, v=1)+P(u=1, v=-1) \\
& =\frac{1}{3}+\frac{1}{3}+\frac{1}{6} \\
& =\frac{5}{6}
\end{aligned}
$$`
---
### Continuous Random Vectors

Let $X$ and $Y$ be two continuous random variables. The random variables $X$ and $Y$ are called (jointly) continuous if the joint distribution function $F(x, y)$ can be expressed by the joint probability density function (joint p.d.f.) $f(x, y)$ as
$$
F(x, y)=\int_{-\infty}^{y} \int_{-\infty}^{x} f(u, v) d u d v
$$
The joint p.d.f. $f(x, y)$ has the following properties:

(i) $f(x, y) \geqslant 0$ for any point $(x, y) \in \mathbb{R} \times \mathbb{R}$;

(ii) $\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x, y) d x d y=1$.

Conversely, if a function $f(x, y)$ satisfies the two properties above, we can prove it must be a joint p.d.f. for some random vector $(X, Y)$.
---
The probability $P[(X, Y) \in A]$ is obtained by integrating the joint p.d.f. over the domain $A$, see Fig. 3.3:
`$$
P[(X, Y) \in A]=\iint_{(x, y) \in A} f(x, y) d x d y
$$`

![](https://pic-1307682807.cos.ap-beijing.myqcloud.com/2023_06_06_f26ecebb4939a04aad18g-09.jpg)
---
Example 3.1.8 Let $X$ and $Y$ have density
$$
f(x, y)= \begin{cases}8 x y & \text { if } 0<y<x<1 \\ 0 & \text { otherwise }\end{cases}
$$
Find $F(x, y)$.

![](https://pic-1307682807.cos.ap-beijing.myqcloud.com/2023_06_06_f26ecebb4939a04aad18g-11.jpg)
---
Finally, for the points $A, B$ and $C$ in Fig. 3.5 satisfying $x \leqslant 0$ or $y \leqslant 0, F(x, y)=0$. For the points $D$ satisfying $0<y<x<1$ in Fig. 3.5,
`$$
F(x, y)=\int_{0}^{y} d v \int_{v}^{x} 8 u v d u=2 x^{2} y^{2}-y^{4}
$$`


For the points $E$ satisfying $0<x<1, y \geqslant x$ in Fig. 3.5,
`$$
F(x, y)=\int_{0}^{x} d u \int_{0}^{u} 8 u v d v=x^{4}
$$`
For the points $F$ satisfying $x \geqslant 1,0 \leqslant y<1$ in Fig. 3.5,
`$$
F(x, y)=\int_{0}^{y} d v \int_{v}^{1} 8 u v d u=2 y^{2}-y^{4} .
$$`
For the points $(x, y)$ satisfying $x>1, y \geqslant 1, F(x, y)=1$. 
---
As a result,
`$$
F(x, y)= \begin{cases}0 & \text { for } x \leqslant 0 \text { or } y \leqslant 0 \\ 2 x^{2} y^{2}-y^{4} & \text { for } 0<x<y<1, \\ x^{4} & \text { for } 0<x<1, y \geqslant x \\ 2 y^{2}-y^{4} & \text { for } x \geqslant 1,0 \leqslant y<1, \\ 1 & \text { for } x>1, y \geqslant 1 .\end{cases}
$$`
						</textarea>
					</section>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## Conditional Distributions

### Discrete case

If $X$ and $Y$ are discrete random variables, then it is natural to define the conditional probability function of $X$ given that $Y=y$, by
$$
p_{X \mid Y}(x \mid y)=P(X=x \mid Y=y)=\frac{P(X=x, Y=y)}{P(Y=y)}=\frac{p(x, y)}{p_{Y}((y)}
$$
for all values of $y$ such that $p_{Y}(y)>0$.

The conditional distribution function of $X$ given that $Y=y$ is defined, for all $y$ such that $p_{Y}(y)>0$, by
$$
F_{X \mid Y}(x, y)=P(X \leqslant x \mid Y=y)=\sum_{x_{i} \leqslant x} p_{X \mid Y}\left(x_{i} \mid y\right)
$$
---
3.18 Suppose that two standard, fair dice are rolled and the sequence of scores $\left(X_{1}, X_{2}\right)$ is recorded. Let $U=\min \left\{X_{1}, X_{2}\right\}$ and $V=\max \left\{X_{1}, X_{2}\right\}$ denote the minimum and maximum scores, respectively.

(a) Find the conditional density of $U$ given $V=v$ for each $v \in\{1,2,3,4,5,6\}$.

(b) Find the conditional density of $V$ given $U=u$ for each $u \in\{1,2,3,4,5,6\}$.
---
Solution. let $U=u$ and $V=v$
$u \leq v$
when $u=v$
Then $P(U=v \cdot v=v)=\frac{1}{6} \times \frac{1}{6}=\frac{1}{36}$
when $u<v$
Then $P(u=u, v: v)=2 \times \frac{1}{6} \times \frac{1}{6}=\frac{1}{18}$
`$$
\begin{align}
&P(v=v)=\frac{1}{6} \times \frac{1}{6}+2 \times \frac{v-1}{6} \times \frac{r}{6}=\frac{2 v-1}{36}\\
&P_{U \mid V}(u \mid v)=\frac{p(U=u, V=v)}{P(V= v)} \\
& = \begin{cases}\frac{1}{2 v-1} &, u=v \\
\frac{2}{2 v-1}&, u<v \\
0 & ,\text { otherwise }\end{cases} \\
\end{align}
$$`

`$$
\begin{aligned}
& P(U=u)=\frac{1}{6} \times \frac{1}{6}+2 \times \frac{6-u}{6} \times \frac{1}{6}=\frac{13-2 u}{36} \\
& P_{V \mid U}(v \mid u)=\frac{P(U= u, U=v)}{P(U=u)} \\
& =\left\{\begin{array}{cc}
\frac{1}{r-2 u} &, u=v \\
\frac{2}{3-2 u} &, u<v \\
0&, \text { otherwise }
\end{array}\right. \\
\end{aligned}
$$`
---
3.20 Suppose that $N$ has the Poisson distribution with parameter 1, and given $N=n, Y$ has the binomial distribution with parameters $n$ and p.

(a) Find the joint probability density function of $(N, Y)$ ?

(b) Find the probability density function of $Y$.

(c) Find the conditional probability density function of $N$ given $Y=k$.
---
Solution. 
`$$
N \sim Poi(1) Y \sim B(n, p)\\
P(N=n)=\frac{1}{e \cdot n !} \quad P(Y=y \mid N=n)=\left(\begin{array}{l}
n \\
y
\end{array}\right) P^y(1-p)^{n-y}
$$`
(a)
`$$
\begin{aligned}
p(n, y) & =P(X=x, Y=y) \\
& =P(Y=y \mid N=n) \cdot P(N=n) \\
& =\frac{1}{e \cdot n !} \cdot \frac{n !}{y !(n-y) !} \cdot P^y(1-p)^{n-y} \\
& =\frac{1}{e \cdot y !(n-y) !} p^y(1-p)^{n-y}
\end{aligned}
$$`
---
(b)
`$$
\begin{aligned}
p_Y(y) & =\sum_{n=0}^{+\infty} p(n, y) \\
& =\frac{p^y}{e \cdot y!} \sum_{n=0}^{+\infty} \frac{(1-p)^{n-y}}{(n-y) !} \\
& =\frac{p^y}{e \cdot y!} \cdot e^{1-p} \\
& =\frac{p^y}{y !} \cdot e^{-p}
\end{aligned}
$$`
---
(C)
`$$
\begin{aligned}
P(N=n \mid Y=k) & =\frac{P(N=n, Y=k)}{P(Y=k)} \\
& =\frac{\frac{1}{e \cdot k ! \cdot(n-k) ! \cdot p^k \cdot(1-p)^{n-k}}}{\frac{p k}{k !} \cdot e^{-p}} \\
& =\frac{(1-P)^{n-k} \cdot e^{p-1}}{(n-k) !}
\end{aligned}
$$`
---
### Continuous Case

If $X$ and $Y$ have joint p.d.f. $f(x, y)$, then the conditional probability density function of $X$ given $Y=y$ is given by
$$
f_{X \mid Y}(x \mid y)= \begin{cases}\frac{f(x, y)}{f_{Y}(y)} & \text { if } 0<f_{Y}(y)<+\infty, \\ 0 & \text { elsewhere. }\end{cases}
$$
---
3.25 Suppose that $X$ is uniformly distributed in the interval $(0,1)$, and that given $X=x$, $Y$ is uniformly distributed in the interval $(0, x)$.

(a) Find the joint density of $(X, Y)$.

(b) Find the probability density function of $Y$.

(c) Find the conditional probability density function of $X$ given $Y=y$ for $y \in(0,1)$.
---
Solution.

`$$
\begin{aligned}
& X \backsim U(0,1) \quad Y \backsim U(0, x) \\
& f_x(x)=\left\{\begin{array}{l}
1,0<x<1 \\
0, \text { others }
\end{array} \quad f_{Y \mid X}(y \mid x)=\left\{\begin{array}{l}
\frac{1}{x}, 0 < y < x \\
0, \text { others }
\end{array}\right.\right.
\end{aligned}
$$`
---
(a)
`$$
\begin{aligned}
f(x, y) & =f_x(x) \cdot f y \mid x(y \mid x) \\
& =\left\{\begin{array}{l}
\frac{1}{x}, y < x < 1 \\
0, \text { others }
\end{array}\right.
\end{aligned}
$$`
---
(b)
`$$
\begin{aligned}
& f_Y(y)=\int_{-\infty}^{+\infty} f(x, y) d x \\
&=\int_y^1 \frac{1}{x} d x \\
&=\left\{\begin{array}{l}
-lny ,0 <y<1 \\
0 \text {, others }
\end{array}\right.
\end{aligned}
$$`
---
(c)
`$$
\begin{aligned}
& f_{X \mid Y}(x \mid y)=\frac{f(x, y)}{f_Y(y)} \\
& =\left\{\begin{array}{l}
-\frac{1}{x lny} ,0 <y<x<1 \\
0 \text {, others }
\end{array}\right.
\end{aligned}
$$`
						</textarea>
					</section>
				</section>

				<section>
					<section data-markdown>
						<textarea data-template>
							## Independence of Random Variables

Given the joint probability (density) function of X*X* and Y*Y*, equations (3.4) and (3.5) yield the marginal probability (density) functions of X*X* and Y*Y*. However, to be given the marginal distributions does not in general uniquely determine a joint distribution. In this section, we will see only if X*X* and Y*Y* are independent, the joint distribution function could be obtained by the marginal distributions.
---
**Definition**

Two random variables $X$ and $Y$ are said to be independent if for every pair of $x$ and $y$,
$$
F(x, y)=F_{X}(x) F_{Y}(y)
$$
We denote it by $X \perp Y$.

Two random variables $X$ and $Y$ are said to be independent if for every pair of $x$ and $y$,
$$
\left\{\begin{aligned}
p(x, y)=p_X(x) \cdot p_Y(y) & \text { when } X \text { and } Y \text { are discrete } \\
& \text { or } \\
f(x, y)=f_X(x) \cdot f_Y(y) & \text { when } X \text { and } Y \text { are continuous. }
\end{aligned}\right.
$$
---
**Theorem**

Let $a, b, c$ and $d$ be given values such that $-\infty \leqslant a<b \leqslant+\infty$ and $-\infty \leqslant c<$ $d \leqslant+\infty$, and let $S$ be a rectangle in the xy-plane:
`$$
S=\{(x, y) \mid a \leqslant x \leqslant b \text { and } c \leqslant y \leqslant d\} 
$$`
Suppose that $f(x, y)=0$ for every point $(x, y)$ outside $S$. Then the continuous (discrete) random variables $X$ and $Y$ are independent if and only if their joint p.d.f. (or p.f.) can be expressed as
$$
f(x, y)=h(x) g(y)
$$
for all points in $S$.

Let $X$ and $Y$ be two independent random variables. Then for any real functions $g(\cdot)$ and $h(\cdot), g(X)$ and $h(Y)$ are independent.
---
						</textarea>
					</section>
				</section>


				<section>
					<section data-markdown>
						<textarea data-template>

## Conditional Distributions

### Discrete case

If $X$ and $Y$ are discrete random variables, then it is natural to define the conditional probability function of $X$ given that $Y=y$, by
`$$
p_{X \mid Y}(x \mid y)=P(X=x \mid Y=y)=\frac{P(X=x, Y=y)}{P(Y=y)}=\frac{p(x, y)}{p_{Y}((y)}
$$`
for all values of $y$ such that $p_{Y}(y)>0$.

The conditional distribution function of $X$ given that $Y=y$ is defined, for all $y$ such that $p_{Y}(y)>0$, by
`$$
F_{X \mid Y}(x, y)=P(X \leqslant x \mid Y=y)=\sum_{x_{i} \leqslant x} p_{X \mid Y}\left(x_{i} \mid y\right)
$$`
---
3.18 Suppose that two standard, fair dice are rolled and the sequence of scores $\left(X_{1}, X_{2}\right)$ is recorded. Let $U=\min \left\{X_{1}, X_{2}\right\}$ and $V=\max \left\{X_{1}, X_{2}\right\}$ denote the minimum and maximum scores, respectively.

(a) Find the conditional density of $U$ given $V=v$ for each $v \in\{1,2,3,4,5,6\}$.

(b) Find the conditional density of $V$ given $U=u$ for each $u \in\{1,2,3,4,5,6\}$.
---
Solution. let $U=u$ and $V=v$
$u \leq v$
when $u=v$
Then $P(U=v \cdot v=v)=\frac{1}{6} \times \frac{1}{6}=\frac{1}{36}$
when $u<v$
Then $P(u=u, v: v)=2 \times \frac{1}{6} \times \frac{1}{6}=\frac{1}{18}$
`$$
\begin{align}
&P(v=v)=\frac{1}{6} \times \frac{1}{6}+2 \times \frac{v-1}{6} \times \frac{r}{6}=\frac{2 v-1}{36}\\
&P_{U \mid V}(u \mid v)=\frac{p(U=u, V=v)}{P(V= v)} \\
& = \begin{cases}\frac{1}{2 v-1} &, u=v \\
\frac{2}{2 v-1}&, u<v \\
0 & ,\text { otherwise }\end{cases} \\
\end{align}
$$`

`$$
\begin{aligned}
& P(U=u)=\frac{1}{6} \times \frac{1}{6}+2 \times \frac{6-u}{6} \times \frac{1}{6}=\frac{13-2 u}{36} \\
& P_{V \mid U}(v \mid u)=\frac{P(U= u, U=v)}{P(U=u)} \\
& =\left\{\begin{array}{cc}
\frac{1}{r-2 u} &, u=v \\
\frac{2}{3-2 u} &, u<v \\
0&, \text { otherwise }
\end{array}\right. \\
\end{aligned}
$$`
---
3.20 Suppose that $N$ has the Poisson distribution with parameter 1, and given $N=n, Y$ has the binomial distribution with parameters $n$ and p.

(a) Find the joint probability density function of $(N, Y)$ ?

(b) Find the probability density function of $Y$.

(c) Find the conditional probability density function of $N$ given $Y=k$.
---
Solution. 
`$$
N \sim Poi(1) Y \sim B(n, p)\\
P(N=n)=\frac{1}{e \cdot n !} \quad P(Y=y \mid N=n)=\left(\begin{array}{l}
n \\
y
\end{array}\right) P^y(1-p)^{n-y}
$$`
(a)
`$$
\begin{aligned}
p(n, y) & =P(X=x, Y=y) \\
& =P(Y=y \mid N=n) \cdot P(N=n) \\
& =\frac{1}{e \cdot n !} \cdot \frac{n !}{y !(n-y) !} \cdot P^y(1-p)^{n-y} \\
& =\frac{1}{e \cdot y !(n-y) !} p^y(1-p)^{n-y}
\end{aligned}
$$`
---
(b)
`$$
\begin{aligned}
p_Y(y) & =\sum_{n=0}^{+\infty} p(n, y) \\
& =\frac{p^y}{e \cdot y!} \sum_{n=0}^{+\infty} \frac{(1-p)^{n-y}}{(n-y) !} \\
& =\frac{p^y}{e \cdot y!} \cdot e^{1-p} \\
& =\frac{p^y}{y !} \cdot e^{-p}
\end{aligned}
$$`
---
(C)
`$$
\begin{aligned}
P(N=n \mid Y=k) & =\frac{P(N=n, Y=k)}{P(Y=k)} \\
& =\frac{\frac{1}{e \cdot k ! \cdot(n-k) ! \cdot p^k \cdot(1-p)^{n-k}}}{\frac{p k}{k !} \cdot e^{-p}} \\
& =\frac{(1-P)^{n-k} \cdot e^{p-1}}{(n-k) !}
\end{aligned}
$$`
---
### Continuous Case

If $X$ and $Y$ have joint p.d.f. $f(x, y)$, then the conditional probability density function of $X$ given $Y=y$ is given by
$$
f_{X \mid Y}(x \mid y)= \begin{cases}\frac{f(x, y)}{f_{Y}(y)} & \text { if } 0<f_{Y}(y)<+\infty, \\ 0 & \text { elsewhere. }\end{cases}
$$
---
3.25 Suppose that $X$ is uniformly distributed in the interval $(0,1)$, and that given $X=x$, $Y$ is uniformly distributed in the interval $(0, x)$.

(a) Find the joint density of $(X, Y)$.

(b) Find the probability density function of $Y$.

(c) Find the conditional probability density function of $X$ given $Y=y$ for $y \in(0,1)$.
---
Solution.
`$$
\begin{aligned}
& X \backsim U(0,1) \quad Y \backsim U(0, x) \\
& f_x(x)=\left\{\begin{array}{l}
1,0<x<1 \\
0, \text { others }
\end{array} \quad f_{Y \mid X}(y \mid x)=\left\{\begin{array}{l}
\frac{1}{x}, 0 < y < x \\
0, \text { others }
\end{array}\right.\right.
\end{aligned}
$$`
---
(a)
`$$
\begin{aligned}
f(x, y) & =f_x(x) \cdot f y \mid x(y \mid x) \\
& =\left\{\begin{array}{l}
\frac{1}{x}, y < x < 1 \\
0, \text { others }
\end{array}\right.
\end{aligned}
$$`
---
(b)
`$$
\begin{aligned}
& f_Y(y)=\int_{-\infty}^{+\infty} f(x, y) d x \\
&=\int_y^1 \frac{1}{x} d x \\
&=\left\{\begin{array}{l}
-lny ,0 <y<1 \\
0 \text {, others }
\end{array}\right.
\end{aligned}
$$`
---
(c)
`$$
\begin{aligned}
& f_{X \mid Y}(x \mid y)=\frac{f(x, y)}{f_Y(y)} \\
& =\left\{\begin{array}{l}
-\frac{1}{x lny} ,0 <y<x<1 \\
0 \text {, others }
\end{array}\right.
\end{aligned}
$$`
						</textarea>
					</section>
				</section>


				<section>
					<section data-markdown>
						<textarea data-template>
## One Function of Two Random Variables

### Discrete Case

If $X$ and $Y$ are independent discrete random variables, then $X+Y$ has probability function
`$$
p_{X+Y}(n)=\sum_{k} p_{X}(k) p_{Y}(n-k)
$$`
The function $p_{X+Y}$ is called the convolution of $p_{X}$ and $p_{Y}$, and is written as
`$$
p_{X+Y}=p_{X} * p_{Y}
$$`
---
Example 3.4.3 Let $X$ and $Y$ be independent geometric variables with common probability function
`$$
p(k)=p(1-p)^{k-1}, \quad k=1,2, \cdots
$$`
Determine the probability function of $X+Y$. 
---
Solution. Because the event $\{X+Y=n\}$ may be written as the union of the disjoint events $\{X=k, Y=n-k\}, 0 \leqslant k \leqslant n$, we have

`$$
\begin{aligned}
P(X+Y=n) & =\sum_{k=1}^{n-1} P(X=k, Y=n-k) \\
& =\sum_{k=1}^{n-1} P(X=k) P(Y=n-k) \\
& =\sum_{k=1}^{n-1} p(1-p)^{k-1} p(1-p)^{n-k-1} \\
& =(n-1) p^{2}(1-p)^{n-2}, \quad n=2,3, \cdots .
\end{aligned}
$$`
Assume $X$ and $Y$ are independent, and $X \sim B\left(n_{1}, p\right), Y \sim B\left(n_{2}, p\right)$. $X+Y \sim B\left(n_{1}+n_{2}, p\right)$.

Assume $X$ and $Y$ are independent, and $X \sim P(\lambda), Y \sim P(\mu)$. Prove $X+Y \sim$ $P(\lambda+\mu)$
---
### Continuous case

Let $X$ and $Y$ be random variables having joint p.d.f. $f(x, y)$. Let $Z$ be given by $Z=\varphi(X, Y)$, where $\varphi$ is a real-valued function whose domain contains the range of $X$ and $Y$. In order to determine the p.d.f. of $Z$, we need to find the d.f. of $Z$ first. For a fixed $z \in \mathbb{R}$, the event $\{Z \leqslant z\}$ is equivalent to the event $\left\{(X, Y) \in A_{z}\right\}$, where $A_{z}$ is the subset of $\mathbb{R}^{2}$ defined by
`$$
A_{z}=\{(x, y) \mid \varphi(x, y) \leqslant z\}
$$`
---
Thus
`$$
\begin{aligned}
F_{Z}(z) & =P(Z \leqslant z)=P(\varphi(X, Y) \leqslant z) \\
& =P\left((X, Y) \in A_{z}\right)=\iint_{A_{z}} f(x, y) d x d y .
\end{aligned}
$$`
If we can find a nonnegative function $g$ such that
`$$
\iint_{A_{z}} f(x, y) d x d y=\int_{-\infty}^{z} g(v) d v, \quad-\infty<z<\infty,
$$`
then $g$ is necessary a density of $Z$.
---
1. The case of $X+Y$

The density of $Z=X+Y$ is given by

`$$
f_{X+Y}(z)=\int_{-\infty}^{\infty} f(x, z-x) d x, \quad-\infty<z<\infty .
$$`

**If $X$ and $Y$ are independent**, then equation (3.16) can be rewritten as
`$$
f_{X+Y}(z)=\int_{-\infty}^{\infty} f_{X}(x) f_{Y}(z-x) d x, \quad-\infty<z<\infty
$$`
---
That means the density of the sum of two independent random variables is the convolution of the individual densities. Equation (3.17) can be written as

$$
f_{X+Y}=f_{X} * f_{Y}
$$

**If $X$ and $Y$ are nonnegative independent random variables**, then $f_{X+Y}(z)=0$ for $z \leqslant 0$ and
`$$
f_{X+Y}(z)=\int_{0}^{z} f_{X}(x) f_{Y}(z-x) d x, \quad 0<z<\infty .
$$`

If $X \sim N\left(\mu_{1}, \sigma_{1}^{2}\right), Y \sim\left(\mu_{2}, \sigma_{2}^{2}\right)$ and $X \perp Y$, then

`$$
a X+b Y+c \sim N\left(a \mu_{1}+b \mu_{2}+c, a^{2} \sigma_{1}^{2}+b^{2} \sigma_{2}^{2}\right)
$$`

where $a, b$ are constants.
---
3.37 Suppose that $X, Y$ and $Z$ are independent and identically distributed random variables, and each has a standard normal distribution. Evaluate $P(3 X+2 Y<6 Z-7)$.
---
Solution.
`$$
\begin{aligned}
& X, Y, Z \sim N(0,1) \\
& P(3 X+2 Y<6 Z-7)=P(3 X+2 Y-6 Z<-7)=P(-3 x-2 Y+6 Z>7)
\end{aligned}
$$`
Let $w=-3 x-2 y+6 z \sim N(0,49)$
`$$
\begin{aligned}
P(3 x+2 y<6 z-7) & =P(-3 x-2 y+6 z>7) \\
& =1-P(-3 x-2 y+6 z \leq 7) \\
& =1-P(w \leq 7) \\
& =1-P\left(\frac{w}{7} \leq \frac{7}{7}\right) \\
& =1-\Phi(1) \\
& =1-0.84124 \\
& =0.15866
\end{aligned}
$$`
---
2. The case of $\max(X,Y)$

For $Z=\max (X, Y)$, we have
`$$
\begin{aligned}
F_{Z}(z) & =P(\max (X, Y) \leqslant z) \\
& =P(\{X \leqslant z, X>Y\} \cup\{Y \leqslant z, X \leqslant Y\}) \\
& =P(X \leqslant z, X>Y)+P(Y \leqslant z, X \leqslant Y) .
\end{aligned}
$$`

Since $\{X>Y\}$ and $\{X \leqslant Y\}$ are mutually exclusive sets that form a partition,

`$$
F_{Z}(z)=P(X \leqslant z, Y \leqslant z)=F(z, z) .
$$`

**If $X$ and $Y$ are independent**, then
`$$
F_{Z}(z)=F_{X}(z) F_{Y}(z)
$$`

and hence
---
`$$
f_{Z}(z)=F_{X}(z) f_{Y}(z)+f_{X}(z) F_{Y}(z) .
$$`

**If $X$ and $Y$ are independent and have identical distribution function $F$ and probability density function $f$**, then equation (3.19) becomes
$$
F_{Z}(z)=[F(z)]^{2}
$$

Equation (3.20) becomes

$$
f_{Z}(z)=2 F(z) f(z)
$$
---
3. The case of $\min(X,Y)$

For $W=\min (X, Y)$, we have

`$$
\begin{aligned}
F_{W}(w) & =P(\min (X, Y) \leqslant w) \\
& =P(\{Y \leqslant w, X>Y\} \cup\{X \leqslant w, X \leqslant Y\}) .
\end{aligned}
$$`

Since the event $\{\min (X, Y) \leqslant w\}$ contains many cases, we consider its complement. Thus

`$$
\begin{aligned}
F_{W}(w) & =1-P(\min (X, Y)>w)=1-P(X>w, Y>w) \\
& =F_{X}(w)+F_{Y}(w)-F_{X, Y}(w, w) .
\end{aligned}
$$`
---
**If $X$ and $Y$ are independent**, then
`$$
F_{W}(w)=1-P(X>w) P(Y>w)=1-\left[1-F_{X}(w)\right]\left[1-F_{Y}(w)\right]
$$`

**If $X$ and $Y$ are independent and have the same distribution function $F$ and probability density function $f$**, then
`$$
F_{W}(w)=1-[1-F(w)]^{2}
$$`

And

`$$
f_{W}(w)=F_{W}^{\prime}(w)=2[1-F(w)] f(w)
$$`
---
Example 3.4.10 Suppose that $X_{1} \sim \operatorname{Exp}(\alpha), X_{2} \sim \operatorname{Exp}(\beta)$ and $X_{1} \perp X_{2}$. Let $Z=\max (X, Y)$ and $W=\min (X, Y)$. Determine the distributions of $Z$ and $W$.
---
Solution. $\quad$ Since $X_{1} \sim \operatorname{Exp}(\alpha)$ and $X_{2} \sim \operatorname{Exp}(\beta)$,

`$$
F_{X}(x)=\left\{\begin{array}{ll}
1-e^{-\alpha x} & \text { for } x>0, \\
0 & \text { for } x \leqslant 0
\end{array} \quad \text { and } \quad F_{Y}(y)= \begin{cases}1-e^{-\beta y} & \text { for } y>0, \\
0 & \text { for } y \leqslant 0\end{cases}\right.
$$`

By using equation (3.19), we get

`$$
F_{Z}(z)=F_{X}(z) F_{Y}(z)= \begin{cases}\left(1-e^{-\alpha z}\right)\left(1-e^{-\beta z}\right) & \text { for } z>0 \\ 0 & \text { for } z \leqslant 0\end{cases}
$$`
---
and hence

`$$
f_{Z}(z)= \begin{cases}\alpha e^{-\alpha z}+\beta e^{-\beta z}-(\alpha+\beta) e^{-(\alpha+\beta) z} & \text { for } z>0 \\ 0 & \text { for } z \leqslant 0\end{cases}
$$`

By using equation (3.23), we can obtain

`$$
F_{W}(w)=1-\left[1-F_{X}(w)\right]\left[1-F_{Y}(w)\right]= \begin{cases}1-e^{-(\alpha+\beta) w} & \text { for } w>0 \\ 0 & \text { for } w \leqslant 0\end{cases}
$$`
---
and hence

$$
f_{W}(w)= \begin{cases}(\alpha+\beta) e^{-(\alpha+\beta) w} & \text { for } w>0 \\ 0 & \text { for } w \leqslant 0\end{cases}
$$

i.e., $W \sim \operatorname{Exp}(\alpha+\beta)$.
						</textarea>
					</section>
				</section>


				<section>
					<section data-markdown>
						<textarea data-template>
							## Numerical Characteristics of Random Variables

### Expectation of Sums and Products

Suppose that $E(X)$ and $E(Y)$ are both finite. We have

`$$
E(X+Y)=E(X)+E(Y) .
$$`

Let $X$ and $Y$ be two independent random variables. Then

`$$
\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)
$$`
---
### Covariance and Correlation

When two random variables $X$ and $Y$ are not independent, it is frequently of interest to assess how strongly they are related to one another. In section 3.6.1, we have

`$$
\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)+2 E[(X-E(X))(Y-E(Y))]
$$`

Now we define the covariance of $X$ and $Y$ by the cross-product term.

The covariance between two random variables $X$ and $Y$ is

`$$
\begin{aligned}
& \operatorname{Cov}(X, Y)=E[(X-E(X))(Y-E(Y))] \\
& = \begin{cases}\sum_{x} \sum_{y}(x-E(X))(y-E(Y)) \cdot p(x, y), & X, Y \text { discrete } \\
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}(x-E(X))(y-E(Y)) \cdot f(x, y) d x d y, & X, Y \text { continuous }\end{cases}
\end{aligned}
$$`
---
Remark: This is the most important in the central joint moments, which are

`$$
\sigma_{i j}=E\left[(X-E(X))^{i}(Y-E(Y))^{j}\right], \quad i, j \geqslant 1
$$`

**The usual way** to calculate the value of covariance $\operatorname{Cov}(X, Y)$ is the following.
`$$
\begin{aligned}
\operatorname{Cov}(X, Y) & =E[X Y-E(X) Y-X E(Y)+E(X) E(Y)] \\
& =E(X Y)-E(X) E(Y)-E(X) E(Y)+E(X) E(Y) \\
& =E(X Y)-E(X) E(Y)
\end{aligned}
$$`
---
For jointly distributed random variables $X$ and $Y$, and constants $a, b$, we have

(i) $\operatorname{Cov}(X, Y)=\operatorname{Cov}(Y, X)$,

(ii) $\operatorname{Cov}(X, X)=E\left(X^{2}\right)-[E(X)]^{2}=\operatorname{Var}(X)$,

(iii) $\operatorname{Cov}(a X, b Y)=a b \operatorname{Cov}(X, Y)$,

(iv) $\operatorname{Cov}(X+Y, Z)=\operatorname{Cov}(X, Z)+\operatorname{Cov}(Y, Z)$.

The correlation coefficient of $X$ and $Y$, denoted by $\rho(X, Y), \rho_{X Y}$, or just $\rho$, is defined by

`$$
\rho(X, Y)=\frac{\operatorname{Cov}(X, Y)}{\sigma_{X} \cdot \sigma_{Y}}
$$`

where $\sigma_{X}$ and $\sigma_{Y}$ are standard deviation of $X$ and $Y$ respectively.
---
**The following statements are equivalent:**

(i) $X$ and $Y$ are uncorrelated,

(ii) $\operatorname{Cov}(X, Y)=0$,

(iii) $E(X Y)=E(X) E(Y)$,

(iv) $\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)$.
---
3.41 Let $X$ be a random variable uniformly distributed in $[0, b]$. Compute $\operatorname{Cov}\left(X, X^2\right)$ and the correlation coefficient $\rho\left(X, X^2\right)$.

`$$
\begin{aligned}
& X \sim U[0, b] \\
& f(x)= \begin{cases}\frac{1}{b}, & 0 \leqslant x \leqslant b \\
0, & \text { otherwise }\end{cases} \\
& \operatorname{Cov}\left(x, x^2\right)=E\left(x \cdot x^2\right)-E(x) \cdot E\left(x^2\right) \\
& \operatorname{Var}(x)=E\left(x^2\right)-E^2(x) \Rightarrow E\left(x^2\right)=\operatorname{Var}(x)+E^2(x)=\frac{b^2}{12}+\left(\frac{b}{2}\right)^2=\frac{1}{3} b^2 \\
& E\left(x^3\right)=\int_{-\infty}^{\infty} x^3 f(x) d x=\int_0^b x^3 \cdot \frac{1}{b} d x=\frac{1}{4} b^3 \\
& \Rightarrow \operatorname{Cov}\left(x, x^2\right)=\frac{1}{4} b^3-\frac{b}{2} \cdot \frac{1}{3} b^2=\frac{1}{12} b^3 \\
\end{aligned}
$$`
---
`$$
\begin{aligned}
& \rho\left(x, x^2\right)=\frac{\operatorname{Con}\left(x, x^2\right)}{\sqrt{\operatorname{Var}(x)} \cdot \sqrt{\operatorname{Var}\left(x^2\right)}} \\
& \operatorname{Var}(x)=\frac{b^2}{12} \\
& \operatorname{Var}\left(x^2\right)=E\left(x^4\right)-E^2\left(x^2\right) \\
& E\left(x^4\right)=\int_{-\infty}^{\infty} x^4 f(x) d x=\int_0^b x^4 \cdot \frac{1}{b} d x=\frac{1}{5} b^4 \\
& \operatorname{Var}\left(x^2\right)=\frac{1}{5} b^4-\frac{1}{9} b^4=\frac{4}{45} b^4 \\
& \Rightarrow P\left(x, x^2\right)=\frac{\frac{1}{12} A^3}{\frac{x}{2 \sqrt{3}} \cdot \frac{2 A^2}{\sqrt{45}}}=\frac{\sqrt{15}}{4} \\
\end{aligned}
$$`
						</textarea>
					</section>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			Reveal.initialize({
				hash: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
